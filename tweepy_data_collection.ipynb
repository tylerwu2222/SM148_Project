{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = \"rs8fimR6iW9hQvLg45utLmdkF\" # REPLACE WITH OWN CONSUMER KEY AND SECRET\n",
    "consumer_secret = \"z19dkW6zpmzBICreE6C0XGCBLJlw64k7byRi58vHYLvzUEqrbi\"\n",
    "  \n",
    "# authorization of consumer key and consumer secret\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "\n",
    "# calling the api \n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API Notes:\n",
    "- User can be suspended, tweet could be deleted -> tweet ID returns error: Forbidden: 403 Forbidden\n",
    "63 - User has been suspended. -> need exception handling\n",
    "- Tweepy's `tweet.user.location` doesn't always return county level\n",
    "- other potential variables of interest: `description` (text of tweet), `followers_count` (to filter for most influential accounts?), `friends_count` (how many following), `created_at`, `id_str` (unique ID for user, could see who's tweeting about COVID the most)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Collection Notes:  \n",
    "\n",
    "0) Download txtcollector to combine csvs for each day into one for month: https://bluefive.pairsite.com/txtcollector.htm\n",
    "1) Download csvs for month using DownGit: https://downgit.github.io/#/home \n",
    "2) combine using txtcollector\n",
    "3) Run below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108663991"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"tweet_data/combinedjuly2021.txt\") # <- change to correct file name\n",
    "total_tweets = len(df.index) # magnitude of ~100 million tweets per month, maybe sample... 100,000? -> filter those for US, ~10,000 per month\n",
    "total_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1421225038431375371, 1416889335052390402, 1412431722856292356, ...,\n",
       "       1419130043285090304, 1417158022900719622, 1419744130893328392],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample from all tweets\n",
    "random.seed(1234)\n",
    "sample_size = 100000\n",
    "rand_tweets_indices = random.sample(range(0,total_tweets),sample_size)\n",
    "tweet_ids = df.iloc[:,0]\n",
    "tweet_id_sample = tweet_ids[rand_tweets_indices].values\n",
    "tweet_id_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter function for US states\n",
    "state_abbrs = [ 'AK', 'AL', 'AR', 'AZ', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', 'GA',\n",
    "           'HI', 'IA', 'ID', 'IL', 'IN', 'KS', 'KY', 'LA', 'MA', 'MD', 'ME',\n",
    "           'MI', 'MN', 'MO', 'MS', 'MT', 'NC', 'ND', 'NE', 'NH', 'NJ', 'NM',\n",
    "           'NV', 'NY', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX',\n",
    "           'UT', 'VA', 'VT', 'WA', 'WI', 'WV', 'WY','US','USA']\n",
    "states = [\"Alaska\", \"Alabama\", \"Arkansas\", \"American Samoa\", \n",
    "            \"Arizona\", \"California\", \"Colorado\", \"Connecticut\", \"District of Columbia\",\n",
    "            \"Delaware\", \"Florida\", \"Georgia\", \"Guam\", \"Hawaii\", \"Iowa\", \"Idaho\", \n",
    "            \"Illinois\", \"Indiana\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Massachusetts\", \n",
    "            \"Maryland\", \"Maine\", \"Michigan\", \"Minnesota\", \"Missouri\", \"Mississippi\", \n",
    "            \"Montana\", \"North Carolina\", \"North Dakota\", \"Nebraska\", \"New Hampshire\", \"New Jersey\", \n",
    "            \"New Mexico\", \"Nevada\", \"New York\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \n",
    "            \"Puerto Rico\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \n",
    "            \"Utah\", \"Virginia\", \"Virgin Islands\", \"Vermont\", \"Washington\", \"Wisconsin\", \"West Virginia\", \n",
    "            \"Wyoming\",]\n",
    "            \n",
    "def filter_location(location):\n",
    "    if ',' not in location:\n",
    "        return False\n",
    "    in_state_abbr = [re.search(', ' + re.escape(state_abbr.lower()) + '$',location.lower()) for state_abbr in state_abbrs] # only County, STATE_ABBR\n",
    "    in_state = [re.search(', ' + re.escape(state.lower()) + '$',location.lower()) for state in states] # only County, state_name.lower()\n",
    "    return any(in_state_abbr + in_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5787\n"
     ]
    }
   ],
   "source": [
    "# test API\n",
    "id_t =  '1487802028394901508'\n",
    "status = api.get_status(id_t)\n",
    "user = status.user\n",
    "print(status.favorite_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# testing filter function\n",
    "print(filter_location('Portland, OR'))\n",
    "print(filter_location('Somewhere, India'))\n",
    "print(filter_location('USA'))\n",
    "print(filter_location('Somwhere, ORCA'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet 0 ; 17.0 pct done\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'all_rows' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-4bb28cde7fa3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m500\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tweet\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\";\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m17000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0msample_size\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"pct done\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mUS_tweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_rows\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mUS_tweets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tweet_data/US_covid_tweets.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# change this to reflect year_month\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# get tweet from id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_rows' is not defined"
     ]
    }
   ],
   "source": [
    "# loop through tweet id samples -> filter further\n",
    "all_rows = []\n",
    "columns = ['location','date','likes','retweets','text']\n",
    "\n",
    "for i,id in enumerate(tweet_id_sample): # may need to check tweet_id?\n",
    "    # save to csv every 500 tweets scanned in case of failure\n",
    "    if i % 500 == 0:\n",
    "        print(\"tweet\",i, \";\",round((i)/sample_size * 100,4), \"pct done\")\n",
    "        US_tweets = pd.DataFrame(all_rows,columns=columns)\n",
    "        US_tweets.to_csv('tweet_data/US_covid_tweets.csv',index=False) # change this to reflect year_month\n",
    "    # get tweet from id\n",
    "    try:\n",
    "        status = api.get_status(id, tweet_mode='extended')\n",
    "        locale = status.user.location\n",
    "        # only append if user location in US\n",
    "        if filter_location(locale):\n",
    "            # if retweet, then need to get full text from retweeted_status (og text is truncated in retweet)\n",
    "            if hasattr(status,'retweeted_status'):\n",
    "                full = status.retweeted_status\n",
    "                row=[locale, status.created_at.strftime(\"%Y-%m-%d\"),full.favorite_count, full.retweet_count, full.full_text]\n",
    "            # else, get from status\n",
    "            else:\n",
    "                row=[locale, status.created_at.strftime(\"%Y-%m-%d\"),status.favorite_count, status.retweet_count, status.full_text]\n",
    "            all_rows.append(row)\n",
    "        \n",
    "    except: # usually tweet deleted or user suspended\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5773\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>likes</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Minnesota, USA</td>\n",
       "      <td>287188</td>\n",
       "      <td>RT @B52Malmet: CDC confirms second US case of ...</td>\n",
       "      <td>2020-01-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>822</td>\n",
       "      <td>RT @ReutersBiz: Coronavirus outbreak may disru...</td>\n",
       "      <td>2020-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>California, USA</td>\n",
       "      <td>21186</td>\n",
       "      <td>#Russia has just closed border due to #Coronav...</td>\n",
       "      <td>2020-01-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Swamp, DC</td>\n",
       "      <td>88859</td>\n",
       "      <td>RT @AP: BREAKING: Delta Air Lines and American...</td>\n",
       "      <td>2020-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Rockford, IL</td>\n",
       "      <td>480</td>\n",
       "      <td>China Lockdown Spreads To 33 Million People As...</td>\n",
       "      <td>2020-01-24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           location   likes  \\\n",
       "0    Minnesota, USA  287188   \n",
       "4       Chicago, IL     822   \n",
       "5   California, USA   21186   \n",
       "9     The Swamp, DC   88859   \n",
       "13     Rockford, IL     480   \n",
       "\n",
       "                                                 text        date  \n",
       "0   RT @B52Malmet: CDC confirms second US case of ...  2020-01-24  \n",
       "4   RT @ReutersBiz: Coronavirus outbreak may disru...  2020-01-28  \n",
       "5   #Russia has just closed border due to #Coronav...  2020-01-30  \n",
       "9   RT @AP: BREAKING: Delta Air Lines and American...  2020-01-31  \n",
       "13  China Lockdown Spreads To 33 Million People As...  2020-01-24  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Don't need to run this; just correcting a mistake\n",
    "valid_locations = US_tweets['location'].apply(lambda x: filter_location(x))\n",
    "US_tweets = US_tweets.loc[valid_locations]\n",
    "print(len(US_tweets.index)) # end up with 5.7k tweets\n",
    "US_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same here; testing\n",
    "# fix_df = pd.read_csv('tweet_data/US_tweets_2020_01.csv')\n",
    "# fix_df.head()\n",
    "num_tweets = len(fix_df.index)\n",
    "print(num_tweets)\n",
    "\n",
    "all_rows = []\n",
    "fixed_cols = ['id','location','text','favorites','retweets','quotes','replies']\n",
    "# full archive search setup here: https://developer.twitter.com/en/account/environments # LIMITED TO 50 requests\n",
    "for i,tweet in fix_df.head(1).iterrows():\n",
    "    if i % 100 == 0:\n",
    "        print(\"tweet\",i,\";\",round(i/num_tweets * 100,4), \"pct done\")\n",
    "        \n",
    "    date_str = datetime.strftime(datetime.strptime(tweet.date,\"%m/%d/%Y\"),\"%Y%m%d\")\n",
    "    tweet_text = tweet.text.replace(':','')[:128]\n",
    "    # print(tweet.location,tweet.date)\n",
    "    results = api.search_full_archive(label = 'individualTweetSearch', query=tweet_text, fromDate=date_str+'0000',toDate=date_str+'2359')\n",
    "    for r in results:\n",
    "        user = r.user\n",
    "        full = r.retweeted_status\n",
    "        if user.location == tweet.location: # found match\n",
    "            # new row\n",
    "            row = [r.id,user.location,full.extended_tweet.get('full_text'),\n",
    "                full.favorite_count,full.retweet_count,full.quote_count,full.reply_count]\n",
    "            all_rows.append(row)\n",
    "            # break when found\n",
    "            break\n",
    "fixed_df = pd.DataFrame(all_rows,columns=fixed_cols)\n",
    "fixed_df.to_csv('fixed_df.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "16e3b9799879e24d92f4a6010a8e3548b08661ef551ca0704bdd958592137b13"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
